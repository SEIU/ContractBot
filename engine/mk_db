#!/usr/bin/env python
"""
Reset ChromaDB and add documents to the database.
"""

from glob import glob
from hashlib import sha1
import json
import os
from pathlib import Path
import sys
from time import time

import chromadb
import click
from tqdm import tqdm

BATCH_SIZE = 100
CONTEXT_SETTINGS = dict(help_option_names=["-h", "-?", "--help"])


def get_metadata(document: Path) -> str:
    useful_keys = {
        "city": "City",
        "cname": "Document Type",
        "company_name": "Company",
        "division": "Division",
        "effective_date": "Effective Date",
        "employer_name": "Employer",
        "expiration_date": "Expiration",
        "local_now": "Local",
        "membership_size": "Local Size",
        "non_profit_ind": "Nonprofit (Y/N)",
        "notes": "Notes",
        "old_local_name": "Prior Local Number",
        "pdf_unique_name": "Document Name",
        "pub_jur_level": "pub_jur_level",  # Not sure of meaning
        "pub_pri_ind": "pub_pri_ind",  # Not sure of meaning
        "state": "State",
        "unit_size": "Unit Size",
        "upload_time": "Uploaded At",
        "Year": "Year",
        "Org": "Org",
        "Topics": "Topics",
    }
    meta_file = document.with_suffix(".meta")
    metadata: str = ""
    if meta_file.exists():
        meta_items = []
        all_meta = json.loads(meta_file.read_text())
        for k, v in all_meta.items():
            if k in useful_keys and v:  # Key is useful and has a value
                meta_items.append(f"{useful_keys[k]}: {v.removesuffix('.pdf')}")
        metadata = "\n".join(meta_items)

    if "Document Name" not in metadata:
        metadata += f"\nDocument Name: {document.name.removesuffix(".txt")}"
    return metadata


def add_document(
    collection: chromadb.Collection,
    file_path: str,
    context_sentences: int = 5,
    context_increment: int = 2,
) -> tuple[chromadb.Collection, int]:
    """
    Chunk each full document into "paragraph" pieces.  A paragraph is
    simply several "sentences" (defined by ending periods) with overlap
    between adjacent chunks.

    Vectorize each chunk and add it to the vector DB collection.

    The chunks are deduplicated by creating a set of them, hence the order
    in which they are processed is indeterminate within a document.

    TODO: A better splitter/chunker is almost surely using LangChain
    https://gist.github.com/tazarov/e66c1d3ae298c424dc4ffc8a9a916a4a
    """
    n_chunks = 0
    doc_file = Path(file_path)
    document = doc_file.read_text()
    sentences = document.replace("\n", " ").split(".")
    metadata = get_metadata(doc_file)

    chunks = set()
    for i in range(0, len(sentences) - context_sentences, context_increment):
        paragraph = ".".join(sentences[i : i + context_sentences])
        content = metadata + "\n.....\n" + paragraph
        chunks.add(content)
    all_chunks = list(chunks)
    all_ids = [sha1(chunk.encode()).hexdigest() for chunk in all_chunks]

    desc = doc_file.name.removesuffix(".txt")[:40].ljust(40)
    for n in tqdm(range(len(chunks) // BATCH_SIZE), desc=desc):
        chunks = all_chunks[n * BATCH_SIZE : (n + 1) * BATCH_SIZE]
        ids = all_ids[n * BATCH_SIZE : (n + 1) * BATCH_SIZE]
        collection.add(documents=chunks, ids=ids)
        n_chunks += len(chunks)

    return collection, n_chunks


@click.command(context_settings=CONTEXT_SETTINGS)
@click.option(
    "-x", "--reset", is_flag=True, help="Reset the vector database of document chunks"
)
@click.option(
    "-f",
    "--filelist",
    type=str,
    help="File with list of files to process, one per line",
)
@click.option(
    "-c",
    "--collection",
    type=str,
    default="BossBot",
    help="Name of collection to create (default 'BossBot')",
)
@click.option(
    "-m",
    "--cosine",
    is_flag=True,
    help="Use cosine similarity metric not squared L2 norm",
)
@click.option(
    "-n",
    "--num-docs",
    type=int,
    default=sys.maxsize,
    help="Limit processing to N files",
)
@click.option(
    "-p",
    "--pattern",
    type=str,
    help="Generate 'documents.cfg' from glob pattern",
)
def main(
    reset: bool,
    filelist: str,
    collection: str,
    num_docs: int,
    pattern: str,
    cosine: bool,
) -> tuple[int, int]:
    client = chromadb.PersistentClient()
    n_chunks = 0
    n_processed = 0
    start = time()

    if reset:
        if os.getenv("ALLOW_RESET") != "TRUE":
            print("DB re-creation permitted only with environment ALLOW_RESET=TRUE")
            sys.exit(-1)
        client.reset()  # Clear the database

    if glob and not filelist:
        files = glob(pattern)
        _filelist: str = "documents.cfg"
        Path("documents.cfg").write_text("\n".join(files))

    if filelist:
        _filelist = filelist
        files = Path(filelist).read_text().splitlines()
        try:
            if cosine:
                collection_obj = client.create_collection(
                    name=collection, metadata={"hnsw:space": "cosine"}
                )
            else:
                # Euclidian distance (the default)
                collection_obj = client.create_collection(
                    name=collection, metadata={"hnsw:space": "l2"}
                )
        except:
            if cosine:
                print("Using existing collection, cosine metric argument ignored")
            collection_obj = client.get_collection(name=collection)

        i = 0
        for document in files:
            i += 1
            if document.startswith("[DONE]"):
                print(f"Existing document {document.removeprefix('[DONE] ')}")
                num_docs += 1  # Add one to get to desired newly processed
                continue
            collection_obj, count = add_document(collection_obj, document)
            n_chunks += count
            n_processed += 1
            # Mark each completed file back to the filelist
            files[i - 1] = f"[DONE] {files[i-1]}"
            Path(_filelist).write_text("\n".join(files))
            if i >= num_docs:
                break

    print(
        f"Processed {n_chunks} chunks from {n_processed} documents "
        f"in {time()-start:,.0f} seconds"
    )
    return n_chunks, n_processed


if __name__ == "__main__":
    main()
