#!/usr/bin/env python
"""
Reset ChromaDB and add documents to the database.
"""

from glob import glob
from hashlib import sha1
import json
import os
from pathlib import Path
import sys
from time import time

import chromadb
import click
from tqdm import tqdm

BATCH_SIZE = 100
CONTEXT_SETTINGS = dict(help_option_names=["-h", "-?", "--help"])


def collection_info(client):
    for c in client.list_collections():
        collection_obj = client.get_collection(c)
        metric = collection_obj.configuration_json["hnsw_configuration"]["space"]
        count = collection_obj.count()
        print(f"{c}: {count:,} vectors, using {metric} metric")


def collection_delete(client, collection):
    confirm = input("Do you really want to delete? [type name of collection again] ")
    if confirm == collection:
        try:
            client.delete_collection(confirm)
            print(f"The collection {confirm} was deleted")
        except ValueError:
            print(f"The collection {confirm} does not exist")


def get_metadata(document: Path) -> str:
    useful_keys = {
        "city": "City",
        "cname": "Document Type",
        "company_name": "Company",
        "division": "Division",
        "effective_date": "Effective Date",
        "employer_name": "Employer",
        "expiration_date": "Expiration",
        "federation": "Federation",
        "local_now": "Local",
        "membership_size": "Local Size",
        "non_profit_ind": "Nonprofit (Y/N)",
        "notes": "Notes",
        "old_local_name": "Prior Local Number",
        "pdf_unique_name": "Document Name",
        "pub_jur_level": "pub_jur_level",  # Not sure of meaning
        "pub_pri_ind": "pub_pri_ind",  # Not sure of meaning
        "state": "State",
        "union": "Union",
        "unit_size": "Unit Size",
        "upload_time": "Uploaded At",
        "Year": "Year",
        "Org": "Org",
        "Topics": "Topics",
    }
    meta_file = document.with_suffix(".meta")
    metadata: str = ""
    if meta_file.exists():
        meta_items = []
        all_meta = json.loads(meta_file.read_text())
        for k, v in all_meta.items():
            if k in useful_keys and v:  # Key is useful and has a value
                meta_items.append(f"{useful_keys[k]}: {v.removesuffix('.pdf')}")
        metadata = "\n".join(meta_items)  # type: ignore

    if "Document Name" not in metadata:
        metadata += f"\nDocument Name: {document.name.removesuffix('.txt')}"
    return metadata


def add_document(
    collection: chromadb.Collection,
    file_path: str,
    context_sentences: int = 5,
    context_increment: int = 2,
) -> chromadb.Collection:
    """
    Chunk each full document into "paragraph" pieces.  A paragraph is
    simply several "sentences" (defined by ending periods) with overlap
    between adjacent chunks.

    Vectorize each chunk and add it to the vector DB collection.

    The chunks are deduplicated by creating a set of them, hence the order
    in which they are processed is indeterminate within a document.

    TODO: A better splitter/chunker is almost surely using LangChain
    https://gist.github.com/tazarov/e66c1d3ae298c424dc4ffc8a9a916a4a
    """
    doc_file = Path(file_path)
    document = doc_file.read_text()
    sentences = document.replace("\n", " ").split(".")
    metadata = get_metadata(doc_file)

    chunks = set()
    for i in range(0, len(sentences) - context_sentences, context_increment):
        paragraph = ".".join(sentences[i : i + context_sentences])
        content = metadata + "\n.....\n" + paragraph
        chunks.add(content)
    all_chunks = list(chunks)
    all_ids = [sha1(chunk.encode()).hexdigest() for chunk in all_chunks]

    desc = doc_file.name.removesuffix(".txt")[:40].ljust(40)
    for n in tqdm(range(len(chunks) // BATCH_SIZE), desc=desc):
        chunk_list = all_chunks[n * BATCH_SIZE : (n + 1) * BATCH_SIZE]
        ids = all_ids[n * BATCH_SIZE : (n + 1) * BATCH_SIZE]
        collection.add(documents=chunk_list, ids=ids)

    return collection


@click.command(context_settings=CONTEXT_SETTINGS)
@click.option(
    "--reset", is_flag=True, help="Reset the vector database of document chunks"
)
@click.option(
    "-f",
    "--filelist",
    type=str,
    help="File with list of files to process, one per line",
)
@click.option(
    "-c",
    "--collection",
    type=str,
    default="BossBot",
    help="Name of collection to create (default 'BossBot')",
)
@click.option(
    "-l",
    "--list-collections",
    is_flag=True,
    help="List the existing collections and their sizes",
)
@click.option(
    "--delete-collection",
    is_flag=True,
    help="Delete a collection (requires confirmation)",
)
@click.option(
    "--cosine",
    is_flag=True,
    help="Use cosine similarity metric not squared L2 norm",
)
@click.option(
    "--inner",
    is_flag=True,
    help="Use inner product metric not squared L2 norm",
)
@click.option(
    "-n",
    "--num-docs",
    type=int,
    default=sys.maxsize,
    help="Limit processing to N files",
)
@click.option(
    "-p",
    "--pattern",
    type=str,
    help="Generate 'documents.cfg' from glob pattern",
)
@click.option("-v", "--verbose", is_flag=True, help="More verbose output")
def main(
    reset: bool,
    filelist: str,
    collection: str,
    list_collections: bool,
    delete_collection: bool,
    cosine: bool,
    inner: bool,
    num_docs: int,
    pattern: str,
    verbose: bool,
) -> int | None:
    client = chromadb.PersistentClient()
    n_processed = 0
    start = time()

    if reset:
        if os.getenv("ALLOW_RESET") != "TRUE":
            print("DB re-creation permitted only with environment ALLOW_RESET=TRUE")
            return -1
        client.reset()  # Clear the database

    if list_collections:
        collection_info(client)
        return 0
    elif delete_collection:
        collection_delete(client, collection)
        return 0

    if pattern:
        files = glob(pattern)
        Path("documents.cfg").write_text("\n".join(files))
        print(f"Created a list of {len(files):,} files in 'document.cfg'")
        return 0

    if not filelist:
        print(f"Run {Path(__file__).name} --help for options")
        return -1
    else:
        files = Path(filelist).read_text().splitlines()
        try:
            if cosine:
                print("WARNING: cosine is not supported by default embedding")
                collection_obj = client.create_collection(
                    name=collection, metadata={"hnsw:space": "cosine"}
                )
            elif inner:
                print("WARNING: inner-product is not supported by default embedding")
                collection_obj = client.create_collection(
                    name=collection, metadata={"hnsw:space": "inner"}
                )
            else:
                # Euclidian distance (the default)
                collection_obj = client.create_collection(
                    name=collection, metadata={"hnsw:space": "l2"}
                )
        except Exception:
            if cosine or inner:
                print(
                    "Using existing collection, "
                    "cosine/inner product metric argument ignored"
                )
            collection_obj = client.get_collection(name=collection)

        i = 0
        for document in files:
            i += 1
            if document.startswith("[DONE]"):
                if verbose:
                    print(f"Existing document {document.removeprefix('[DONE] ')}")
                num_docs += 1  # Add one to get to desired newly processed number
                continue
            collection_obj = add_document(collection_obj, document)
            n_processed += 1
            # Mark each completed file back to the filelist
            files[i - 1] = f"[DONE] {files[i-1]}"
            Path(filelist).write_text("\n".join(files))
            if i >= num_docs:
                break

        print(f"Processed {n_processed} documents in {time()-start:,.0f} seconds")
        n_vec = collection_obj.count()  # type: ignore
        print(f"Collection {collection} has {n_vec:,.0f} vectors/chunks")
        return n_processed


if __name__ == "__main__":
    main()
